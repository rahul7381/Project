{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PART - 1**"
      ],
      "metadata": {
        "id": "hA7Pe2QhLVo_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWkq_lL-HmSo",
        "outputId": "96c4cf4d-c080-4bd9-e403-f8fb0f866de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 1 transactions saved to transactions_db1.csv\n",
            "Database 2 transactions saved to transactions_db2.csv\n",
            "Database 3 transactions saved to transactions_db3.csv\n",
            "Database 4 transactions saved to transactions_db4.csv\n",
            "Database 5 transactions saved to transactions_db5.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# List of items\n",
        "items = [\n",
        "    \"Diapers\",\n",
        "    \"Milk\",\n",
        "    \"Bread\",\n",
        "    \"Eggs\",\n",
        "    \"Apples\",\n",
        "    \"Cereal\",\n",
        "    \"Toilet Paper\",\n",
        "    \"Pasta\",\n",
        "    \"Soap\",\n",
        "    \"Chips\"\n",
        "]\n",
        "\n",
        "# Generate transactions for multiple databases\n",
        "num_databases = 5\n",
        "num_transactions_per_db = 20\n",
        "\n",
        "for db_id in range(1, num_databases + 1):\n",
        "    transactions = []\n",
        "    for transaction_id in range(1, num_transactions_per_db + 1):\n",
        "        transaction_date = (date.today() - timedelta(days=random.randint(1, 30))).strftime(\"%Y-%m-%d\")\n",
        "        item = random.choice(items)\n",
        "        quantity = random.randint(1, 5)\n",
        "        price = round(random.uniform(1.0, 10.0), 2)\n",
        "        transactions.append((transaction_id, transaction_date, item, quantity, f\"${price:.2f}\"))\n",
        "\n",
        "    # Save transactions to CSV\n",
        "    csv_filename = f\"transactions_db{db_id}.csv\"\n",
        "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Transaction ID\", \"Date\", \"Item\", \"Quantity\", \"Price\"])\n",
        "        writer.writerows(transactions)\n",
        "\n",
        "    print(f\"Database {db_id} transactions saved to {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART - 2**"
      ],
      "metadata": {
        "id": "kKNqthWgLa61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Sample items (you can replace with your actual items)\n",
        "items = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "\n",
        "# Generate all possible 1-itemsets\n",
        "one_itemsets = [(item,) for item in items]\n",
        "\n",
        "# Generate all possible 2-itemsets\n",
        "two_itemsets = list(combinations(items, 2))\n",
        "\n",
        "# Generate all possible 3-itemsets\n",
        "three_itemsets = list(combinations(items, 3))\n",
        "\n",
        "# Assume some sample transactions (you can replace with your actual data)\n",
        "transactions = [\n",
        "    [\"A\", \"B\", \"C\"],\n",
        "    [\"A\", \"C\", \"D\"],\n",
        "    [\"B\", \"C\", \"E\"],\n",
        "    # ... add more transactions\n",
        "]\n",
        "\n",
        "# Calculate support threshold (you can adjust this based on your data)\n",
        "support_threshold = 2\n",
        "\n",
        "# Check if each itemset is frequent\n",
        "frequent_itemsets = []\n",
        "for itemset in one_itemsets + two_itemsets + three_itemsets:\n",
        "    count = sum(1 for transaction in transactions if all(item in transaction for item in itemset))\n",
        "    if count >= support_threshold:\n",
        "        frequent_itemsets.append((itemset, count))\n",
        "\n",
        "# Print frequent itemsets\n",
        "for itemset, count in frequent_itemsets:\n",
        "    print(f\"Frequent itemset {itemset}: Support count = {count}\")\n",
        "\n",
        "# Generate association rules (you can extend this part)\n",
        "for itemset, count in frequent_itemsets:\n",
        "    if len(itemset) > 1:\n",
        "        for i in range(1, len(itemset)):\n",
        "            antecedent = itemset[:i]\n",
        "            consequent = itemset[i:]\n",
        "            print(f\"Association rule: {antecedent} => {consequent}\")\n",
        "\n",
        "# Repeat for higher k-itemsets if needed\n",
        "\n",
        "# Note: This is a simplified example; adapt it to your actual data and requirements.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJBt5KMlIAD5",
        "outputId": "9a80e73f-6e98-4f85-8c92-4569cf2214d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent itemset ('A',): Support count = 2\n",
            "Frequent itemset ('B',): Support count = 2\n",
            "Frequent itemset ('C',): Support count = 3\n",
            "Frequent itemset ('A', 'C'): Support count = 2\n",
            "Frequent itemset ('B', 'C'): Support count = 2\n",
            "Association rule: ('A',) => ('C',)\n",
            "Association rule: ('B',) => ('C',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART - 3**"
      ],
      "metadata": {
        "id": "f507dmwDLcxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from efficient_apriori import apriori\n",
        "from fpgrowth_py import fpgrowth\n",
        "import time\n",
        "\n",
        "# User input: Set support and confidence thresholds\n",
        "min_support = 0.1  # Adjust as needed\n",
        "min_confidence = 0.5  # Adjust as needed\n",
        "\n",
        "# Load transaction data (replace with your actual data)\n",
        "def load_transactions(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    # Ensure all items are strings\n",
        "    transactions = [set(str(item) for item in row.dropna()) for _, row in df.iterrows()]\n",
        "    return transactions\n",
        "\n",
        "transactions_db1 = load_transactions(\"transactions_db1.csv\")\n",
        "transactions_db2 = load_transactions(\"transactions_db2.csv\")\n",
        "# Repeat for other databases\n",
        "\n",
        "# Apriori algorithm\n",
        "start_time = time.time()\n",
        "itemsets_apriori, rules_apriori = apriori(transactions_db1, min_support=min_support, min_confidence=min_confidence)\n",
        "end_time = time.time()\n",
        "print(f\"Apriori execution time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# FP-growth algorithm\n",
        "start_time = time.time()\n",
        "freqItemSet, rules_fpgrowth = fpgrowth(transactions_db1, minSupRatio=min_support, minConf=min_confidence)\n",
        "end_time = time.time()\n",
        "print(f\"FP-growth execution time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Print association rules\n",
        "print(\"\\nApriori Rules:\")\n",
        "for rule in rules_apriori:\n",
        "    print(rule)\n",
        "\n",
        "print(\"\\nFP-growth Rules:\")\n",
        "for rule in rules_fpgrowth:\n",
        "    print(rule)\n",
        "\n",
        "# Compare results (you can add more comparison logic if needed)\n",
        "if len(rules_apriori) == len(rules_fpgrowth):\n",
        "    print(\"\\nBoth algorithms produced the same number of rules.\")\n",
        "else:\n",
        "    print(\"\\nAlgorithms produced different numbers of rules.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dwmm5qGICvF",
        "outputId": "5d6fdaae-a59f-4aab-fad6-f4355ba6726b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apriori execution time: 0.0007 seconds\n",
            "FP-growth execution time: 0.0007 seconds\n",
            "\n",
            "Apriori Rules:\n",
            "{2024-02-22} -> {1} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{1} -> {2024-02-22} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{Toilet Paper} -> {2} (conf: 0.500, supp: 0.100, lift: 2.000, conv: 1.500)\n",
            "{3} -> {2024-02-22} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{2024-02-22} -> {3} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{Soap} -> {3} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{3} -> {Soap} (conf: 0.667, supp: 0.100, lift: 4.444, conv: 2.550)\n",
            "{Pasta} -> {4} (conf: 1.000, supp: 0.100, lift: 2.857, conv: 650000000.000)\n",
            "{Cereal} -> {5} (conf: 0.500, supp: 0.100, lift: 1.667, conv: 1.400)\n",
            "\n",
            "FP-growth Rules:\n",
            "[{'Pasta'}, {'4'}, 1.0]\n",
            "[{'3'}, {'2024-02-22'}, 0.6666666666666666]\n",
            "[{'2024-02-22'}, {'3'}, 0.6666666666666666]\n",
            "[{'Soap'}, {'3'}, 0.6666666666666666]\n",
            "[{'3'}, {'Soap'}, 0.6666666666666666]\n",
            "\n",
            "Algorithms produced different numbers of rules.\n"
          ]
        }
      ]
    }
  ]
}